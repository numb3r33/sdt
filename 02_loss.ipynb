{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Decision Tree Loss\n",
    "\n",
    "Train the soft decision tree using a loss function that seeks to minimize the cross entropy between each leaf, weighted by its path probability, and the target distribution. For a single training case with input vector x and target distribution `T`, the loss is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![loss](images/loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid getting stuck at poor solutions during the training, authors introduced a\n",
    "penalty term that encouraged each internal node to make equal use of both\n",
    "left and right sub-trees. Without this penalty, the tree tended to get stuck on\n",
    "plateaus in which one or more of the internal nodes always assigned almost all\n",
    "the probability to one of its sub-trees and the gradient of the logistic for this\n",
    "decision was always very close to zero. The penalty is the cross entropy between\n",
    "the desired average distribution 0.5, 0.5 for the two sub-trees and the actual\n",
    "average distribution $\\alpha$,(1 âˆ’ $\\alpha$) where $\\alpha$ for node i is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![regularizer](images/regularizer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "IN_COLAB = 'google.colab' in str(get_ipython())\n",
    "if IN_COLAB:\n",
    "  !pip3 install -Uqq fastbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/github/sdt\n",
      "/content/drive/My Drive/Colab Notebooks/github/sdt\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "if IN_COLAB:\n",
    "  from pathlib import Path\n",
    "  from nbdev.imports import Config\n",
    "  project_path = Path('/content/drive/My Drive/Colab Notebooks/github/sdt')\n",
    "  get_ipython().magic(f'cd {project_path}')\n",
    "  get_ipython().magic(f'cd {Config().nbs_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "if IN_COLAB:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SDTLoss(Module):\n",
    "  def __init__(self, lambda_):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.lambda_ = lambda_\n",
    "  \n",
    "  def set_path_prob(self, path_prob): self.path_prob = path_prob\n",
    "  def set_regularizer(self, numers, denoms): self.numers, self.denoms = numers, denoms\n",
    "  \n",
    "  def forward(self, output, target):\n",
    "    # number of target categories\n",
    "    target_ohe = torch.zeros((output.shape[0], output.shape[-1])).cuda()\n",
    "\n",
    "    target = target.view(-1, 1)\n",
    "\n",
    "    # assert to find out whether shape of target \n",
    "    # is similar to output or not\n",
    "    target_ohe.scatter_(1, target, 1).cuda()\n",
    "    target_ohe = target_ohe.unsqueeze(dim=2)\n",
    "\n",
    "    log_output = torch.log(output)\n",
    "    res = torch.bmm(log_output, target_ohe).squeeze(dim=2)\n",
    "\n",
    "    # weigh cross entropy over all paths\n",
    "    res = (self.path_prob * res).sum(dim=1)\n",
    "\n",
    "    # calculate regularizer\n",
    "    alphas = self.numers.sum(dim=0) / self.denoms.sum(dim=0)\n",
    "    lambdas_ = torch.ones_like(alphas)\n",
    "    \n",
    "    for i in range(int(np.log2(len(lambdas_) + 1))):\n",
    "      start_index = 2 ** i - 1\n",
    "      end_index   = 2 ** i\n",
    "      lambdas_[start_index:start_index+end_index] = 2 ** (-i)\n",
    "    \n",
    "    lambdas_ = self.lambda_ * lambdas_\n",
    "    \n",
    "    reg_inner_term = (0.5 * torch.log(alphas) + torch.log(1 - alphas) * 0.5) * lambdas_\n",
    "    C = -reg_inner_term.sum()\n",
    "    \n",
    "    return -res.mean() + C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.ipynb.\n",
      "Converted 01_model.ipynb.\n",
      "Converted 02_loss.ipynb.\n",
      "Converted 03_train.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
